{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \\\n",
        "  langchain==0.2.16 \\\n",
        "  langchain-core==0.2.41 \\\n",
        "  langchain-community==0.2.16 \\\n",
        "  langchain-google-genai==1.0.10 \\\n",
        "  faiss-cpu newspaper3k textblob tiktoken wikipedia duckduckgo-search sentence-transformers lxml_html_clean\n"
      ],
      "metadata": {
        "id": "y5PI3YbI-Qzu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "wKpkJ-1_DhCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, hashlib\n",
        "from typing import Dict, Any\n",
        "\n",
        "from newspaper import Article\n",
        "from textblob import TextBlob\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n"
      ],
      "metadata": {
        "id": "lwDC8kSEJRa8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) NewsSenseAI/1.0\"\n",
        "\n",
        "print(\"âœ… Keys loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy69YPK_GzND",
        "outputId": "ed0a6352-e3a2-4111-88ad-b6944ad071a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Keys loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Models in fallback order (most quota-friendly first)\n",
        "MODEL_FALLBACK = [\n",
        "    \"models/gemini-flash-lite-latest\",\n",
        "    \"models/gemini-2.0-flash-lite\",\n",
        "    \"models/gemini-2.0-flash\",\n",
        "    \"models/gemini-pro-latest\"\n",
        "]\n",
        "\n",
        "# âœ… RAG configuration (token efficient)\n",
        "CHUNK_SIZE = 600\n",
        "CHUNK_OVERLAP = 100\n",
        "TOP_K = 2  # keep low to reduce tokens\n",
        "\n",
        "# âœ… In-memory cache to avoid repeated API calls\n",
        "CACHE: Dict[str, Any] = {}\n"
      ],
      "metadata": {
        "id": "vPondHIxId0_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cache_key(*parts) -> str:\n",
        "    raw = \"||\".join([str(p) for p in parts])\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def load_article_text(url: str):\n",
        "    \"\"\"\n",
        "    BBC-safe article extraction\n",
        "    \"\"\"\n",
        "    art = Article(url)\n",
        "    art.download()\n",
        "    art.parse()\n",
        "    title = art.title.strip()\n",
        "    text = art.text.strip()\n",
        "    return title, text\n",
        "\n",
        "def sentiment_offline(text: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Offline sentiment with TextBlob\n",
        "    \"\"\"\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        tone = \"Positive ðŸ˜Š\"\n",
        "    elif polarity < -0.1:\n",
        "        tone = \"Negative ðŸ˜ \"\n",
        "    else:\n",
        "        tone = \"Neutral ðŸ˜\"\n",
        "    return {\"tone\": tone, \"polarity\": float(polarity)}\n"
      ],
      "metadata": {
        "id": "06yKN6piKwWE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_call(prompt: str, model_name: str, temperature=0):\n",
        "    llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "def safe_gemini(prompt: str, cache_id: str, temperature=0, max_retries=6):\n",
        "    \"\"\"\n",
        "    Optimal Gemini caller:\n",
        "    âœ… caching\n",
        "    âœ… fallback models\n",
        "    âœ… retry with exponential backoff\n",
        "    \"\"\"\n",
        "    if cache_id in CACHE:\n",
        "        return CACHE[cache_id]\n",
        "\n",
        "    last_err = None\n",
        "\n",
        "    for model_name in MODEL_FALLBACK:\n",
        "        delay = 5\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                out = gemini_call(prompt, model_name=model_name, temperature=temperature)\n",
        "                CACHE[cache_id] = out\n",
        "                return out\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                msg = str(e)\n",
        "\n",
        "                # rate limit / quota\n",
        "                if (\"429\" in msg) or (\"ResourceExhausted\" in msg):\n",
        "                    print(f\"âš ï¸ 429 rate limit on {model_name}. Sleeping {delay}s...\")\n",
        "                    time.sleep(delay)\n",
        "                    delay = min(delay * 2, 60)\n",
        "                    continue\n",
        "\n",
        "                # model not supported\n",
        "                if (\"404\" in msg) or (\"not found\" in msg):\n",
        "                    print(f\"âš ï¸ Model not supported: {model_name}. Trying next model...\")\n",
        "                    break\n",
        "\n",
        "                # other error\n",
        "                break\n",
        "\n",
        "    raise RuntimeError(f\"âŒ Gemini failed after retries/models. Last error: {last_err}\")\n"
      ],
      "metadata": {
        "id": "lO9FnKoaL51R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vectorstore(article_text: str):\n",
        "    docs = [Document(page_content=article_text)]\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "    splits = splitter.split_documents(docs)\n",
        "\n",
        "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vs = FAISS.from_documents(splits, emb)\n",
        "    return vs\n"
      ],
      "metadata": {
        "id": "UqrTL9bhL-GQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_article(vectordb, url: str) -> str:\n",
        "    \"\"\"\n",
        "    One-shot RAG summary:\n",
        "    âœ… retrieve top chunks\n",
        "    âœ… single Gemini call\n",
        "    \"\"\"\n",
        "    # retrieve once (no Gemini here)\n",
        "    docs = vectordb.similarity_search(\"Give a concise summary of the article\", k=TOP_K)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are NewsSense AI.\n",
        "Summarize the following news article using ONLY the context.\n",
        "\n",
        "Rules:\n",
        "- EXACTLY 3 bullet points\n",
        "- Each bullet must be max 2 lines\n",
        "- No extra headings/text\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\"\n",
        "    cache_id = make_cache_key(url, \"summary\", TOP_K, CHUNK_SIZE)\n",
        "    return safe_gemini(prompt, cache_id)\n",
        "\n",
        "def extract_claims(vectordb, url: str) -> str:\n",
        "    \"\"\"\n",
        "    Optional claim extraction: 1 Gemini call\n",
        "    \"\"\"\n",
        "    docs = vectordb.similarity_search(\"Extract fact-checkable claims\", k=TOP_K)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Extract 5 fact-checkable claims from the news context below.\n",
        "Rules:\n",
        "- Output ONLY numbered list (1..5)\n",
        "- Each claim short & verifiable\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\"\"\"\n",
        "    cache_id = make_cache_key(url, \"claims\", TOP_K, CHUNK_SIZE)\n",
        "    return safe_gemini(prompt, cache_id)\n"
      ],
      "metadata": {
        "id": "Guf80uvYMASd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(vectordb, url: str, question: str) -> str:\n",
        "    docs = vectordb.similarity_search(question, k=TOP_K)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer the question using ONLY the context.\n",
        "If the answer is not in context, say: \"Not found in article.\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\"\"\"\n",
        "    cache_id = make_cache_key(url, \"qna\", question, TOP_K, CHUNK_SIZE)\n",
        "    return safe_gemini(prompt, cache_id)\n"
      ],
      "metadata": {
        "id": "Ow7V2DfFMCCr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_news(url: str, include_claims=False) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Main pipeline:\n",
        "    âœ… article load\n",
        "    âœ… vector index\n",
        "    âœ… 1-call summary\n",
        "    âœ… offline sentiment\n",
        "    âœ… optional claims\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ”„ Processing URL: {url}\")\n",
        "\n",
        "    # Article load cached (avoid re-downloading)\n",
        "    cache_id = make_cache_key(url, \"article\")\n",
        "    if cache_id in CACHE:\n",
        "        title, text = CACHE[cache_id]\n",
        "    else:\n",
        "        title, text = load_article_text(url)\n",
        "        CACHE[cache_id] = (title, text)\n",
        "\n",
        "    if len(text) < 400:\n",
        "        return {\"error\": \"âŒ Article text too short / blocked\"}\n",
        "\n",
        "    print(\"âœ… Article loaded:\", title)\n",
        "\n",
        "    # Vectorstore cached\n",
        "    vs_key = make_cache_key(url, \"vectorstore\", CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "    if vs_key in CACHE:\n",
        "        vectordb = CACHE[vs_key]\n",
        "    else:\n",
        "        vectordb = build_vectorstore(text)\n",
        "        CACHE[vs_key] = vectordb\n",
        "        print(\"âœ… Indexed: FAISS ready\")\n",
        "\n",
        "    # Summary (1 Gemini call)\n",
        "    summary = summarize_article(vectordb, url)\n",
        "\n",
        "    # Sentiment offline\n",
        "    senti = sentiment_offline(text[:3000])\n",
        "\n",
        "    output = {\n",
        "        \"title\": title,\n",
        "        \"summary\": summary,\n",
        "        \"sentiment\": senti,\n",
        "        \"vectorstore\": vectordb  # keep for QnA\n",
        "    }\n",
        "\n",
        "    # Optional claims (1 Gemini call)\n",
        "    if include_claims:\n",
        "        output[\"claims\"] = extract_claims(vectordb, url)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "PWBuAYAEMGGi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_to_test = \"https://www.bbc.com/news/articles/cn5ln4wee7wo\"\n",
        "\n",
        "res = analyze_news(url_to_test, include_claims=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“° NEWS REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(\"TITLE:\", res.get(\"title\"))\n",
        "\n",
        "print(\"\\nâœ… SUMMARY:\\n\", res.get(\"summary\"))\n",
        "print(\"\\nâœ… SENTIMENT:\\n\", res.get(\"sentiment\"))\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mu64z6vMI2M",
        "outputId": "bbcb9084-124a-4245-82df-bf7ad96305f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Processing URL: https://www.bbc.com/news/articles/cn5ln4wee7wo\n",
            "âœ… Article loaded: Aravalli: Why India's ancient hills are at the centre of growing protests\n",
            "âœ… Indexed: FAISS ready\n",
            "\n",
            "============================================================\n",
            "ðŸ“° NEWS REPORT\n",
            "============================================================\n",
            "TITLE: Aravalli: Why India's ancient hills are at the centre of growing protests\n",
            "\n",
            "âœ… SUMMARY:\n",
            " * Activists demand the government define Aravalli areas using scientific criteria like geography and ecology.\n",
            "* A critical role in ecology or preventing desertification should be recognized as part of the range, regardless of height.\n",
            "* Critics warn the court's new definition could encourage mining, construction, and commercial activity, risking ecological damage.\n",
            "\n",
            "âœ… SENTIMENT:\n",
            " {'tone': 'Neutral ðŸ˜', 'polarity': 0.07667189132706373}\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"vectorstore\" in res:\n",
        "    q = \"where the protest is happening?\"\n",
        "    ans = ask_question(res[\"vectorstore\"], url_to_test, q)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ¤– RAG Q&A\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Q:\", q)\n",
        "    print(\"A:\", ans)\n",
        "    print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KOJbQgLML9F",
        "outputId": "c0afc3b1-d7f4-4483-efcd-6b871a2174ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ¤– RAG Q&A\n",
            "============================================================\n",
            "Q: where the protest is happening?\n",
            "A: Gurugram city near Delhi\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y0zZV9wM5du"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}